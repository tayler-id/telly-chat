# YouTube Video Analysis

**Date:** 6/19/2025

### üìπ Video Information
- **Video ID:** `-cRedoYETzQ`
- **Language:** en

### üìù Full Transcript

```
A few years ago, training in 16bit precision was state-of-the-art. Then Deepseek pulled off 8bit training and now we are pushing the limits with only four bits. This sounds absurd because four-bit gradients are brutally coarse and in theory models shouldn't even converge. Yet, this very new paper claims that they trained an entire model in floating point 4 and matched the precision of a floatingoint 16 baseline. But of course, there's more to the story than the headline reveals. This video is part of a longer series on quantization, but is fully self-contained. We'll talk about how Nvidia's Tensor Core supports mixed precision training, the new floatingoint formats designed specifically for quantization values, and the modeling tricks that make 4-bit gradients work. As always, you'll find my reading list and the slide deck on Patreon for free. So, let's dive in. We're all very much aware that training LLM is really expensive. A Stamford report estimated that training Gemini Ultra must have cost a little under $200 million in compute and GBT4 about half of that. Though Sam Alman claims even that is an underestimation and more recently the anthropic CEO revealed that training claw 3.7 costs a few tens of millions of dollars. Research labs from China like Deepseek and Quen managed to reduce the cost of training LLMs by a factor of 10, though it's still in the millions. Much of Deepseek V3's cost efficiency comes from pushing all the heavy matrix multiplications down to floating point.8 when most of the industry was still working with floating point 16. This drops the overall memory requirements by 40% and increased training speed by a factor of 1.8x. That's why historically the number of bits used during training has been steadily decreasing. In the '90s at the dawn of deep learning, we started with floating point 32. At the time, this was the right compromise between memory usage and the precision required for gradient descent. But models kept increasing in parameter count and putting pressure on memory. So in 2018, researchers at Google Brain unlocked training in 16 bits with their Bflat 16 format. This was a modification to the de facto ILE E standard, making a trade-off that was more suited to deep learning. While 8-bit training had been explored for a while, it wasn't until 2024 that DeepSync V3 became the first highquality LLM to actually make it work in production. This established a new trend which was also embraced by Meta with their Llama 4 model. And now here we are pushing the limits with just four bits. Something that once sounded impossible but might soon be the new normal. Now, you might think that the switch from one precision to the next was clear-cut, but in practice, as we go down the timeline, we gradually swap more and more high precision components for lower precision ones. The result is a mixture of precisions within the same model. For instance, the Deepseek V3 model, recognized as the first production quality LLM to train predominantly in floating point. 8 describes a mixed precision framework that includes floating point 8, 16, and even 32. The paper we're discussing in this video uses bolder language, floating point 4 all the way, but in reality, it doesn't escape mixed precision. Its training strategy is very similar to DeepSeek, except that it targets four bits instead of eight. So, it's more like floating point for some of the way. I know I'd be a terrible PR person, but I will cave into peer pressure and call it fully quantized training for the rest of the video. So, mix precision is inevitable, at least for now, because LM components have various degrees of tolerance to noise. Some operations still require full precision. embedding text tokens into vectors, calculating attention scores, or applying soft max to compute output probabilities. However, matrix multiplications are a lot more resilient to the loss in precision. Also, they're everywhere deeply embedded in the transformer block. In a 30 billion parameter LLM, they account for 90% of the computational cost. So making them faster is great return over investment. That's why fully quantized training focuses on quantizing matrix multiplications between weights, activations, and even gradients. Now gradients, those are really surprising. If you watch my previous video on one bit LLMs, you've already seen low precision weights and activations during the quantization aware training process. But how on earth can we do gradient descent with just four bits or 16 representable values? Well, I'd say it's a team effort. It's the confluence of innovations from three different directions. Hardware, numeric formats, and machine learning modeling tricks. Let's start from the base of the tech stack, the hardware. Meet Tensor. The reason why Nvidia is a trillion dollar company today. Here's how Nvidia describes them. So tensor corores are the workhorse of mixed precision training. What tensor core really does is it performs a matrix multiply accumulate. So why that matters is matrix multiplies are the basis of the really heavy workloads and deep learning. So tensor takes in 16- bit floating point and then does this matrix multiply and then accumulates everything in 32-bit and 32-bit precision accumulation um tends to really matter for convergence of networks to really make mixed precision work. So tensor corores perform blazingly fast matrix multiplication which is nice of course but what's really relevant here is the mixed precision part which means lower precision input matrices like floating point 4 and higher precision output matrices like floating point 8. This is important in order to prevent overflow. Multiplying two matrices means taking the dotproduct of every row in X and every column in Y. And for big matrices, we could be summing over thousands of 4-bit elements and will require larger bit widths to store the output. Tensor cores handle this mixed precision natively. They were introduced back in 2017 with the Volta GPUs just as deep learning started to take off. Correlation or causation? I guess we'll never know. With every new architecture, Nvidia expanded support for more data formats. Hopper introduced native support for floating.8 with accumulation and 32bits. This is the architecture behind the famous H100 GPU. Ongoing US China export controls would in theory prevent DeepSeek from acquiring them, but a recent congressional report from April 2025 suggests that they might have procured H100s through indirect channels. Anyway, now with Blackwell, the latest generation, we get native support for floating point. 4, though these trips are currently very hard to procure. To understand how hardware accelerates fully quantized training, let's zoom in on the architecture of an Nvidia GPU. While some details are Nvidia specific, the core ideas apply across most vendors. The fundamental compute unit is the streaming multipprocessor or SM. Think of it as the GPU equivalent of a CPU core. An H100 has 132 SM all sharing a global memory pool of 80 GB. Inside each SM, there are two main types of compute units. 64 CUDA cores for general purpose operations and four tensor cores for high throughput matrix multiplications. All cores in an SM share a block of onchip memory called shared memory which is faster and higher bandwidth than the global memory. To keep things simple, I'm skipping over deeper levels of memory hierarchy like caches and registers. And also these numbers are specific to the H100. Blackwell specs aren't fully public yet, but this gives you a sense of the general scale. Since linear layers are at the core of fully quantized training, let's look at how they map onto Nvidia GPUs. A linear layer L takes activations from a previous layer L minus one, multiplies them by a matrix WL, and produces the output activation AL. That's the forward pass. In training, we also have a backward pass where gradients G flowed down the network. The first step is to assign blame. We compute a delta, the gradient of the loss with respect to the weights. This tells us how to nudge the weights to reduce the loss. Then we propagate that blame down, computing how the loss depends on the previous layer's activations. These two gradient calculations might look intimidating, but they're just matrix multiplications between gradients, activations, and weights. It's a simple application of the chain rule. So if you've seen standard gradient descent before, nothing here should feel new. Fully quantized training simply aims to perform these three matrix multiplications as efficiently as possible. The three matrix multiplications are executed by the tensor cores. After all, this is exactly what tensor cores were designed for. The weights live in the global memory in full precision. And finally, CUDA core applies the model updates and executes any other model operations that are not matrix multiplications. So when they say floating point4 all the way, what they actually mean is that all matrix multiplications inside of tensor core happen in floating point 4. All the matrices including the weights, activations from the layer below, and gradients from the layer above are all in floating point 4. This is the key. This is what makes training a lot faster. While Tensor Core is a very efficient matrix multiplier, it's bottlenecked by memory. And bringing down inputs from floating point 32 to floating point 4 reduces pressure 8 times. So how do we bring them down to floating point 4? Let's start with the weights. Remember the master copy lives in the global memory in full precision. CUDA core reads the matrix from there, quantizes it into floating point 4 and places it into the shared memory from where tensor core can access it. We'll discuss the specifics of the quantization process in a moment. Now, what about the input activations and gradients? How do they end up in floating point 4? Well, the neighbors of our linear layer L might or might not be linear layers themselves. So when activations and gradients flow through the network, they do so in high precision, be it floating point 32 or maybe BF16, you can think of this high precision as a handshake or glue between potentially heterogeneous layers. This means CUDA core will have to quantize them just like it did with the weights. So we cover the inputs to tensor core, but what about the outputs? Well, remember Blackwell supports native floating point 4 operations with accumulation and floating point.8. CUDA core is free to upcast these values as needed. For instance, when applying the model updates, it reads delta from the shared memory in floating point.8 as tensor core produced it and then upcasts it to floating point 32 to match the precision of the master weights. So this is fully quantized training in a nutshell. If you watched my previous video, you might be wondering how it's different from quantization aware training or QAT, which is the training method used by one bit LLM. Conceptually, QAT performs the backward pass as usual in full precision, but makes the forward pass aware of quantization. So internally weights and activations are quantized, multiplied and then dequantized back before leaving the linear layer. This gives the model exposure to the loss and precision caused by quantization. However, the operants themselves never leave the floating point 32 format. So weights will temporarily be binary, but their minus1 and +1 values are still stored in 32 bits. So all tensor core multiplications are done in high precision regardless of the simulated quantization. This is why QAT does not lead to more efficient training the way fully quantized training does. Now we're ready to talk about the second major ingredient that makes floatingpoint training work. New data formats specifically designed to hold quantized values. Over the past few years, researchers have proposed custom low precision formats and simulated them in software. But now there are actually standards ready to be embraced by chip manufacturers. Hardware companies like Nvidia, AMD, Intel, and Qualcomm came together to propose microscaling or MX data formats. The microscaling alludes to the quantization scale. If you watched my videos on quantization fundamentals, you're already familiar with it. If not, here's a quick recap. For simplicity, let's quantize a float 16 value into an int4. The same principles apply when quantizing into floating point 4 or some other format. Let's say our floats range from minus1 to + one and our integer buckets from -7 to + 7. To map one range into the other, we compute the scale, which is a ratio. The width of the float interval over the width of the integer interval. It basically measures how big one space is compared to the other. Then to quantize the value r, we simply need to divide it by the scale and round it to the nearest bucket. This gives us a straightforward quantization function q in terms of the scale s. So this is the quantization scale that microscaling refers to. Another important aspect of the MX standard is that scaling happens per block. Let's see what that means. Here we are looking at the weight matrix. To quantize it, we could find the minimum alpha and maximum beta in the matrix, compute the scale S, and then apply the function Q to every element. Implicitly, we're making the decision to treat the entire matrix as a monolith. In contrast, per block quantization splits the matrix into multiple blocks and then computes the scale S for each block independently. The size of the block is like a knob. A small block size means more blocks and higher precision, but it is less memory efficient because we need to store more real values. S. This blockbased approach is what microscaling formats are built around. Normally when we talk about floatingoint formats like ILE E, we're describing a single value with a sign, an exponent, and the mantisa. But a single microscaling value like MXFP4 encapsulates multiple values namely 32 4-bit values for the quantized values in the block and a single 8 bit value for the scale. The block values are in E2 M1 format. That means two bits for the exponent, one bit for the mantisa. Now, the scale in the MX FP4 format uses all its bits for the exponent. There's no sign and no mantisa. That's a pretty opinionated choice. So, Nvidia offers their own alternative format called NVFB4 with a scale that uses four bits for the exponent and three for the mantisa. The floating point for all the way paper finds that the Nvidia format leads to slightly better model performance. So native support for this rather complex data format means addition and multiplications with accumulation in floating point.8 are implemented directly in hardware and we don't need to apply the scale manually during these operations. All this magic happens inside tensor cores. Before wrapping up our conversation on microscaling formats, I wanted to address a question that I had the first time I read about this topic. Why use floating point for instead of int? After all, both formats give us 16 discrete values to work with. And in fact, there are older papers that train directly in int4. To understand why floating point4 is better, let's go back to our quantization exercise. Mapping float 16 values into 4-bit integers. To make things easier to see, I'll rescale the real values interval. With int four, we divide this space into equally sized bins. If our real values were uniformly distributed, this would be optimal. However, model weights tend to follow a normal distribution clustered tightly around zero. The result is middle bins get overs subscribed and outer bins barely get utilized. We're basically misallocating our resources. In contrast to integers, floatingoint representations can spread their values unevenly. For instance, this is the normal float 4 format or NF4 introduced in the Q Laura paper. It consists of 16 values handpicked in a way that is theoretically optimal for quantizing normally distributed real values. Think of it as a lookup table where each bin gets its own 4bit encoding. But unfortunately this doesn't map well to existing hardware. The microscaling format strikes a different trade-off. Its block elements which follow the E2M1 format might not be as nicely distributed but come with real floatingoint behavior. Only 11 out of the 16 combinations are valid numbers and the rest are special values infinities and nans. But special values are not a waste. They signal things like underflows, overflows, and undefined operations, which in a real training loop can be very useful. Finally, let's talk about the modeling tricks that enable training in floating point 4. Across the literature, you will find the grab bag of creative solutions that deal with various quantization related challenges like activation outliers, non-ifferiability of the rounding function or low precision gradients. However, the dust hasn't settled yet and none of these tricks are universally accepted. So, instead of dumping an exhaustive list on you, I will talk about one of the tricks used in the floating point for all the way paper that I think is here to stay. The challenge they address is the inherent bias of the rounding function used during gradient quantization. So, what does bias mean? Imagine you're trying to walk in a straight line. Each step is a little off. Sometimes to the left, sometimes to the right, but on average, the deviations cancel out and you still reach the destination. Now imagine every step is just slightly off to the right. The total error might be the same, but you'll drift off course. That's the danger of biased gradient quantization during training. Not that it makes big errors, but it nudges the model consistently in the wrong direction. I'll illustrate it with integer quantization for clarity, but it applies for any quantization space. Normally, for the rounding function, we use round to nearest or RTN. We'll pick whichever bucket is closest to the real value. Now, say our gradients often land near the edge of a bucket, just shy of the next one. In that case, round to nearest consistently rounds them down, introducing bias in the learning process. Intuitively, we can tell something is off because the last bin remains unused. To circumvent this problem, we could use stochastic rounding instead. For each gradient value, we toss a coin on whether to run it up or down. This is the equivalent of sometimes stepping left and sometimes stepping right but eventually reaching the destination. I posted this animation a few days ago and asked you guys what metaphor it could be in the context of quantization. So shout out to Michael and Bobby who did get it right despite very little context. And that's how fully quantized training went from research curiosity to a fast approaching reality. So, how good are these models trained natively in floating point 4? Well, I couldn't find any pre-trained checkpoints out there. There are open source repositories, but they mostly just contain the code to train the models as opposed to actual checkpoints. So, the best we can do is look at the results reported in the paper. First, we see that the two training losses when training with floating point 4 and BF-16 are on par. There's a very small gap that can be closed with just a few steps of quantization aware fine-tuning. Again, that's when the backwards pass is done in full precision. The evaluation benchmarks show a similar pattern where floating point 4 and Bflat 16 models are very close. However, the authors seem to have cherrypicked classification style tasks. Most of them, with the exception of Lambata, contain multiplechoice questions as opposed to requiring fullyfledged language generation. So, we learned that training in floating. 4 is technically possible, though we haven't quite reached a deepseek moment that completely changes how the industry operates. However, as Blackwell chips become more available, it's probably just a matter of time until fully quantized training with four bits becomes the standard. If you want to dive deeper, I've shared my slide deck and my entire reading list on Patreon for free. As we're approaching the end of the quantization series, I'm very curious to know what other topics you'd like me to cover. So, let me know in the comments. Thanks for watching and I'll see you next time.
```

### üìã Action Plan

Based on the video transcript about quantized training for Large Language Models (LLMs), here's a comprehensive action plan:

SUMMARY:
The video explains how deep learning training has evolved from 32-bit to 4-bit precision, focusing on how modern systems achieve efficient training through mixed precision and specialized hardware. It details Nvidia's Tensor Core architecture and how different precision levels are used for different components of LLM training.

PREREQUISITES:
- Understanding of basic deep learning concepts
- Knowledge of matrix multiplication operations
- Familiarity with GPU architecture basics
- Access to modern GPU hardware (preferably Nvidia)

DETAILED ACTION PLAN:

1. Setup Phase (1-2 weeks)
   - Acquire appropriate GPU hardware (preferably Nvidia H100 or newer)
   - Install required deep learning frameworks
   - Set up development environment with mixed precision support

2. Model Architecture Adaptation (2-3 weeks)
   - Identify components for different precision levels:
     * Keep embedding layers in FP16/32
     * Convert matrix multiplications to lower precision
     * Maintain softmax in higher precision
   - Implement mixed precision training framework
   - Set up gradient scaling mechanisms

3. Training Implementation (3-4 weeks)
   - Configure tensor cores for mixed precision operations
   - Implement gradient accumulation in FP32
   - Set up proper memory management systems
   - Create monitoring systems for numerical stability

4. Optimization Phase (2-3 weeks)
   - Fine-tune quantization parameters
   - Implement gradient clipping
   - Optimize memory usage patterns
   - Configure proper batch sizes for stability

EXPECTED OUTCOMES:
- Reduced memory usage by approximately 40%
- Increased training speed by ~1.8x
- Maintained model accuracy comparable to FP16 training
- Lower training costs

COMMON PITFALLS TO AVOID:
1. Don't quantize all components uniformly
2. Avoid aggressive quantization of critical operations
3. Watch for numerical instability in gradients
4. Don't ignore hardware-specific optimizations
5. Maintain higher precision for sensitive operations

REQUIRED RESOURCES:
- Modern Nvidia GPU (H100 or newer)
- Deep learning framework with mixed precision support
- Monitoring tools for numerical stability
- Sufficient storage for checkpoints
- Memory profiling tools

Note: This plan assumes an intermediate to advanced level of expertise in deep learning and system optimization. Timelines may vary based on experience level and available resources.

---

## Analysis Details

**Tools Used:** youtube_transcript
