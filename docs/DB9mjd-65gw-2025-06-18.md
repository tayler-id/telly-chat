# YouTube Video Analysis

**Date:** 6/18/2025

### üìπ Video Information
- **Video ID:** `DB9mjd-65gw`
- **Language:** en

### üìù Full Transcript

```
Andrew Mayne: Welcome to the OpenAI podcast. Andrew Mayne: My name is Andrew Mayne. Andrew Mayne: For several years, I worked at OpenAI first as an engineer on the applied team and then as the science communicator. Andrew Mayne: After that, I worked with companies and individuals trying to figure out how to incorporate artificial intelligence. Andrew Mayne: With this podcast, we have the opportunity to talk to the people working with and at OpenAI about what's going on behind the scenes and maybe get a glimpse of the future. Andrew Mayne: My first guest is Sam Altman, CEO and co-founder of OpenAI. Andrew Mayne: And we're gonna find out a bit more about Stargate, how he uses ChatGPT as a parent, and maybe get an idea of when GPT-5 is coming. Sam Altman: More and more people will think we've gotten to an AGI system every year. Sam Altman: What you want out of hardware and software is changing quite rapidly. Sam Altman: But if people knew what we could do with more compute, they would want way, way more. Andrew Mayne: One of my friends is a new parent and is using ChatGPT a lot to ask questions. Andrew Mayne: It's become a very good resource. Andrew Mayne: And you are a new parent. Andrew Mayne: And how much has ChatGPT been helping you with that? Sam Altman: A lot. Sam Altman: I mean, clearly people have been able to take care of babies without ChatGPT for a long time. I don't know how I would have done that. Sam Altman: Those first few weeks, it was like every I mean, constantly. Sam Altman: Now I now I kinda ask it questions about like developmental stages more because Andrew Mayne: I can Sam Altman: I can do the basics, but Andrew Mayne: Is this normal? Sam Altman: Yeah. Sam Altman: But it was super helpful for that. Sam Altman: I spend a lot of time thinking about how my kid will use AI in in the future. Sam Altman: It it is sort of like, by the way, extremely kid-pilled. Sam Altman: I think everybody should have a lot of kids. Andrew Mayne: Yeah. Andrew Mayne: A lot of my friends at OpenAI, former colleagues and current ones, are having kids. Andrew Mayne: And people go like, oh, what about this AI thing? Andrew Mayne: Everyone I know inside is very optimistic in having families. Sam Altman: I think that's a good sign. Sam Altman: Yeah. Sam Altman: Like, my kids will never be smarter than AI. Sam Altman: But also they will grow up. Andrew Mayne: Way to set them back Sam Altman: there though. Sam Altman: I mean, they will grow up like vastly more capable than we grew up and able to do things that would just, we cannot imagine. Sam Altman: And they'll be really good at using AI. Sam Altman: And obviously, think about that a lot, but I I think much more about the, like, what they will have that we didn't than what is gonna be taken away. Sam Altman: They're like, I don't don't think my kids will ever be bothered by the fact that they're not smarter than AI. Sam Altman: I just like, you know, I there's this video that always has stuck with me of a baby or like a little toddler with one of those old glossy magazines going like this on the screen. Andrew Mayne: Because it thinks it's an iPad. Andrew Mayne: Thought it was Sam Altman: a broken iPad. Sam Altman: Yeah. Sam Altman: And, you know, kids born now will just think the world always had extremely smart AI. Sam Altman: And they will use it incredibly naturally and they will look back at this as like a very, you know, prehistoric time period. Andrew Mayne: I saw something on social media where a guy talked about he got tired of talking to his kid about Thomas the Tank Engine. Andrew Mayne: So he put it into ChatGPT into voice mode. Sam Altman: Kids love voice mode Andrew Mayne: in ChatGPT. Andrew Mayne: And he was like an hour later, the kid's still talking about Thomas the train. Sam Altman: Again, I suspect there this is not all gonna be good. Sam Altman: There will be problems. Sam Altman: People will develop Andrew Mayne: Yeah. Sam Altman: These sort of somewhat problematic or maybe very problematic parasocial relationships and, well, society will have to figure out new guardrails and but the upsides will be tremendous. Sam Altman: And we society in general is good at figuring out how to mitigate the downsides. Andrew Mayne: Yeah. Andrew Mayne: So, yeah, I think optimistic. Andrew Mayne: We're seeing some interesting data where used along in classrooms with a good teacher, good curriculum, ChatGPT becomes very good, used solely by itself as sort of a homework crutch can lead to kids sort of just doing the same thing as trying to Google stuff. Sam Altman: I was one of those kids that everyone's worried I was just gonna Google everything when it came out and stopped learning. Sam Altman: You know, it turns out, like, relatively quickly, kids in schools adapt. Sam Altman: So Yeah. Sam Altman: I think we'll figure this out. Andrew Mayne: Think of what you could have become if you didn't Google everything, Sam. Andrew Mayne: You know? Andrew Mayne: So we've seen this adoption figures, are really insane. Andrew Mayne: It's OpenAI's most popular product. Andrew Mayne: Five years from now, is it gonna be ChatGPT? Sam Altman: I mean, I think ChatGPT will just be a totally different thing five years from now. Sam Altman: So in some sense, no. Sam Altman: But will it still be called ChatGPT? Sam Altman: Probably. Andrew Mayne: Yeah. Andrew Mayne: Okay. Andrew Mayne: So it's a solid name. Andrew Mayne: So the other thing we hear is AGI, which I'd like to hear your definition of AGI. Sam Altman: In many senses, if you asked me or anybody else to propose a definition of AGI five years ago based off, like, the cognitive capabilities of software. Sam Altman: I think the definition many people would would have given then is now, like, well surpassed. Sam Altman: Like, these models are smart now. Andrew Mayne: Right. Sam Altman: And they'll keep getting smarter. Sam Altman: They'll keep improving. Sam Altman: I think more and more people will think we've gotten to an AGI system every year. Sam Altman: Even though the definition will keep pushing out and getting more ambitious, like, more people will still agree to it. Sam Altman: But, you know, we have systems now that are really increasing people's productivity that are able to do valuable economic work. Sam Altman: Maybe a better question is what will it take for something I would call superintelligence? Andrew Mayne: Okay. Sam Altman: If we had a system that was capable of either doing autonomous discovery of new science or greatly increasing the capability of people using the tool to discover new science, that would feel like kind of almost definitionally superintelligence to me and be a wonderful thing for the world, I think.  Andrew Mayne: So, basically, a lot of it's kind of this gradient where it keeps getting better and better in each one of our definitions. I felt like that way when we hit GPT-4 internally playing at this, this, I'm like, there's ten years of runway that we can do so much stuff with this. Andrew Mayne: And even when it starts using itself, like, can enter reasoning was really capable. Andrew Mayne: But when you're saying it comes up with some new theorem or proof or something, and then, oh, hey. Andrew Mayne: We found a better cure for cancer, or I found out some new GLP drug or something. Sam Altman: Yeah. Sam Altman: I mean, I I am a big believer that the high order bit of people's lives getting better is more scientific progress. Sam Altman: Mhmm. Sam Altman: That is kind of that is kind of what what limits us. Sam Altman: And so if we can discover much more, I I think that really will have a a very significant impact. Sam Altman: And for me, that would just be, like, a tremendously exciting milestone. Sam Altman: I think many other great uses of AI will happen too, but that one feels really important. Andrew Mayne: Have you seen, like, signs of this you'd see internally? Andrew Mayne: Have you seen things that made you go, oh, I think we've kind of figured it out? Sam Altman: Nothing where I would say we have figured it out, but I would say increasing confidence on the directions to pursue. Sam Altman: Maybe the I mean, this is the example everyone talks about, but I think it it is still interesting. Sam Altman: What's happening with people using AI systems to write code and coders being much more productive and thus researchers as well? Sam Altman: Like, that is a sort of example of, okay. Sam Altman: It's obviously not doing new science, but it is definitely making scientists able to do their work faster. Sam Altman: We hear this with o3 all the time from scientists as well. Sam Altman: So I wouldn't say we figured it out. Sam Altman: I wouldn't say we we know the algorithm where we're just like, alright. Sam Altman: We can point this thing, and it'll go do science on its own. Sam Altman: But we're getting good guesses, and the rate of progress is continuing to just be, like, super impressive. Sam Altman: Watching the progress from o1 to o3 where it was like every couple of weeks, the team was just like, we have a major new idea, and they all kept working. Sam Altman: It was a reminder of sometimes when you, like, discover a big new insight, things can go surprisingly fast, and I'm sure we'll see that many more times. Andrew Mayne: I noticed recently OpenAI had just shifted the model in Operator to o3. Andrew Mayne: And I noticed a big improvement Sam Altman: Way better. Andrew Mayne: With operator. Andrew Mayne: It and I'd I'd say that the thing that we ran into before was brittleness, is that you have people who promise agentic systems, can do all these things, but the moment it gets to a problem it can't solve, it falls apart. Sam Altman: Interestingly, speaking of the AGI question, a lot of people have told me that their personal moment was operator with o three, and there's something about watching an AI use a computer pretty well. Sam Altman: Not perfectly, but it's not. Sam Altman: It's o3 was a big step forward that feels very AGI like. Sam Altman: It didn't it didn't really have that effect on me to the same degree, although it's it's quite impressive, but I I've heard that enough times. Andrew Mayne: Mine was with Deep Research because that felt like a really agentic use of it. Andrew Mayne: And that was when I came back and it produced something on a topic because I had been interested in that was better than I read before because previously all those models would just get a bunch of sources, summarize it. Andrew Mayne: But when I watched the system go out on the Internet, get data Yeah. Andrew Mayne: Follow that, then follow that lead, and then follow back, then come back, like I would've, but better, was interesting. Sam Altman: I met this guy recently. Sam Altman: He's like a one of these, like, crazy autodidacts, just obsessed with learning and knows about everything. Sam Altman: And he uses Deep Research to produce a report on anything he's curious about and then just sits there all day and has gotten good at digesting them fast and know what to ask next. Sam Altman: And it is like, it is an amazing new tool for people who really have a crazy appetite to learn. Andrew Mayne: I built my own app that literally lets me ask questions and it generates audio files for me of this stuff because it's just like that. Andrew Mayne: My curiosity probably exceeds my retention. Andrew Mayne: And Operator, I'll tell you the magical moment for me, and I'm curious see where the thing's going next, was I was doing a thing on Marshall McLuhan, and I wanted to get a bunch of images of Marshall McLuhan, and I asked her to do it. Andrew Mayne: And then all of a sudden, I had a whole folder full of these things, was, for a research thing, would have taken me forever to do. Sam Altman: Yeah. Sam Altman: I think we're just gonna keep seeing things like this where whatever we thought about what a workflow had to be like and how long something had to take is gonna just change, like, wildly fast. Andrew Mayne: Yeah. Andrew Mayne: How are you using it? Andrew Mayne: Deep Research? Sam Altman: Yeah. Sam Altman: Science that I'm curious about? Sam Altman: I'm I'm just in this, like, weird place of I am extremely time strapped. Sam Altman: If I had more time, I would read like, I would read Deep Research reports preferentially to reading most other things, but I'm sort of short on time to read in general. Sam Altman: Yeah. Andrew Mayne: What's neat too is the sharing feature, which I love because now it's easy to share that with somebody else. Andrew Mayne: The PDFs are great, and that's cool. Andrew Mayne: And I would say that even though we have deep research, we have these tools, there is a model race going on. Andrew Mayne: And so the question comes up is like GPT-5. Andrew Mayne: And any idea is that with a system like that, we should see an increase in capabilities. Andrew Mayne: What is the time frame for GPT-five? Andrew Mayne: When are we gonna see this? Sam Altman: Probably sometime this summer. Sam Altman: Right. Sam Altman: I don't know exactly when. Sam Altman: One thing that we go back and forth on is how much are we supposed to, like, turn up the big number on new models versus what we did with GPT-4o, which is just better and better and better and Andrew Mayne: I had to handle the release of GPT-4, right, when that was coming out. Andrew Mayne: And meanwhile, I had to kinda do this take test off between that and 3.5. Andrew Mayne: And 3.5 kept getting better and better and better. Andrew Mayne: And the comparisons I was able to make were changing. Andrew Mayne: And so that's my question. Andrew Mayne: It's like, yeah, that, you know, like, would I know GPT-5 versus, wow, this is a really good GPT-4.5? Sam Altman: Not necessarily. Sam Altman: I mean, it, like, it could go either way. Sam Altman: Right? Sam Altman: You could just, like, keep doing iterations of 4.5 or at some point you could call it five. Sam Altman: It used to be much clearer. Sam Altman: We would train a model and put it out, and then we would train a new big model and put it out. Sam Altman: And, you know, now the systems have gotten much more complex, and we can continually post train them to make them better. Sam Altman: I were thinking about this right now. Sam Altman: Like, every time let's say we launch GPT-5, and then we update it and update it and update it. Sam Altman: Should we just keep calling those GPT-5? Sam Altman: Like, we do with GPT-4o, or should we call those 5.1, 5.2, 5.3 so you know which you know when the version changes? Sam Altman: I don't think we have an answer to this yet, but but I think there is something better to do than the way we handled it with 4o. Sam Altman: We we see this periodically. Sam Altman: Like, sometimes people like one snapshot much better than another, and they might wanna keep using one. Sam Altman: We we gotta gotta figure something out here. Andrew Mayne: Yeah. Andrew Mayne: The challenge is even if you're technically inclined, you can kind of understand, okay, if there's an o before it, I know this. Andrew Mayne: But if I want, know, like but then even then it's not clear, should I use o4-mini? Andrew Mayne: Should I use o3? Andrew Mayne: Should I use this? Sam Altman: I think this was like an example of this was an artifact of shifting paradigms. Andrew Mayne: Mhmm. Sam Altman: And then we kinda had these two things going at once. Sam Altman: I think we are near the end of this current problem, but I can imagine a world I don't know what it is, but I can imagine a world that we discover some new paradigm that again means we need to, like, bifurcate the model tree. Andrew Mayne: Okay. Andrew Mayne: Even more complicated names. Sam Altman: I hope we don't have to do that. Sam Altman: I am excited to just get to GPT-5 and GPT-6, and I think that'll be easier for people to use, and you won't have to think, do I want, you know, o4-mini-high or o3 or 4o Andrew Mayne: o4-mini-high is what I use to code. Andrew Mayne: Yeah. Andrew Mayne: I have a conversation, it's o3. Sam Altman: I think we will be out of that whole mess soon Yeah. Sam Altman: For now. Andrew Mayne: Yeah. Andrew Mayne: I mean, it's it's fun to have choice when you know what they mean, but it's it's still I think one of the things that's made these things more capable but also harder to understand where the capability is coming from is integrations of things like memory. Andrew Mayne: And memory started off as one very simple thing and now memory's got more sophisticated. Sam Altman: Memory is probably my favorite recent ChatGPT feature. Sam Altman: Mhmm. Sam Altman: You know, the first time you could talk to a computer, like GPT-3 or whatever, that felt like a really big deal. Sam Altman: And now that the computer I feel like it kind of, like, knows a lot of context on me. Sam Altman: And if I ask it a question with only a small number of words, it knows enough about the rest of my life to be pretty confident in what I want it to do. Sam Altman: Sometimes in ways I don't even think of. Sam Altman: Like, that has been a real surprising, like, level up. Sam Altman: So I and and I hear that from a lot of other people as well. Sam Altman: There are people who don't like it, but most people really do. Sam Altman: I think we are heading towards a world where if you want, the AI will just have, like, unbelievable context on your life and give you these super, super helpful answers. Andrew Mayne: Which I for me is cool. Andrew Mayne: The fact that you can turn it off is also great. Andrew Mayne: But one of the challenges came out was in New York Times ongoing lawsuit with OpenAI, they just asked the court to tell OpenAI they had to preserve consumer ChatGPT user records beyond the 30 day window that has to be held for regular reasons. Andrew Mayne: And Brad Lightcap just wrote a letter responding to this. Andrew Mayne: Could you explain? Sam Altman: We're gonna fight that, obviously, and I suspect, I hope, but I do think we we will win. Sam Altman: I think it was a crazy overreach of the New York Times to ask for that. Sam Altman: This is someone who says, you know, they value user privacy, whatever. Sam Altman: But I to, like, look for the silver lining here. Sam Altman: I hope this will be a moment where society realizes that privacy is really important. Sam Altman: Privacy needs to be a core principle of using AI. Sam Altman: You cannot have a company like The New York Times ask an AI provider to compromise user privacy, And I think society needs to I think it's really unfortunate The New York Times did that, but I hope this accelerates the conversation that society needs to have about how we're going to treat privacy and AI. Sam Altman: And I hope the answer is, like, we take it very, very seriously. Sam Altman: People are having quite private conversations with ChatGPT now. Sam Altman: ChatGPT will be a very sensitive source of information, and I think we need a framework that reflects that. Andrew Mayne: So that brings up the other question from people who are using this or skeptical is that OpenAI now has access to this data, and there's the concern one was about training, which OpenAI has been very clear about when or when not. Andrew Mayne: It's training. Andrew Mayne: You have the option to turn that off. Andrew Mayne: The other thing is, like, advertising, things like that. Andrew Mayne: What's OpenAI's approach towards that? Andrew Mayne: How are you gonna handle that responsibility? Sam Altman: We haven't done any advertising product yet. Sam Altman: I kind of I mean, I'm not totally against it. Sam Altman: I can point to areas where I like ads. Sam Altman: I think ads on Instagram, kinda cool. Sam Altman: I bought a bunch of stuff from them. Sam Altman: But I am like I think it'd be very hard to I mean, take a lot of care to get right. Sam Altman: Yeah. Sam Altman: People have a very high degree of trust in ChatGPT, which is interesting because like AI hallucinates. Sam Altman: It should be the tech that you don't trust that much. Andrew Mayne: My friends hallucinate too, I trust them too Sam Altman: People really Yeah. Sam Altman: Do. Sam Altman: But I think part of that is if you compare us to social media or, you know, web search or something, where you can kinda tell that you are being monetized and the company is trying to, like, deliver you good products and services, no doubt, but also to kind of, like, get you to click on ads or whatever. Sam Altman: Like, you know, how much how much do you believe that, like, you're getting the thing that that company actually thinks is the best content for you versus something that's also trying to, like, interact with the ads? Sam Altman: I think there's, like, there's a psychological thing there. Sam Altman: So, for example, I think if we started modifying the output, like the stream that comes back from the LLM. Sam Altman: In exchange for who is paying us more, that would feel really bad. Sam Altman: Yeah. Sam Altman: And I and I would hate that as a user. Sam Altman: I think that'd be like a trust destroying moment. Sam Altman: Maybe if we just said, hey, we're never gonna modify that stream, but, like, if you click on something in there that is gonna be what we'd show anyway, we'll, like, we'll get, like, a little bit of the transaction revenue, and it's a flat thing for everybody. Sam Altman: If if we, you know, have, like, a easy way to pay for it or something, maybe that could work. Sam Altman: Maybe there could be, like, ads outside the transaction stream. Sam Altman: Sorry, outside of the LLM stream that are still really great. Sam Altman: But the burden of proof there, think would have to be very high and it would have to feel like really useful to users and really clear that it was not messing with the LLM's output. Andrew Mayne: Yeah. Andrew Mayne: It's gonna be a difficult one. Andrew Mayne: I hope there's a solution. Andrew Mayne: I would love to do all my purchasing through ChatGPT or a really good chatbot because a lot of the times I feel like I'm not making the most informed decisions. Andrew Mayne: And so mitigating Sam Altman: Yeah. Sam Altman: No. Sam Altman: That's good if we can do it in some sort of really clear and aligned way, but I don't know. Sam Altman: Like, I love that we build good services. Sam Altman: People pay us for them. Sam Altman: It's like very clear. Sam Altman: It's a Andrew Mayne: Well, that's benefit. Andrew Mayne: That's like, I'd say the difference in models is like, I think Google builds great stuff. Andrew Mayne: I think the new Gemini 2.5 is a really good model. Andrew Mayne: I think they went from Sam Altman: It is a really good model. Andrew Mayne: Yeah. Andrew Mayne: They went from kinda like, and like, oh, man, these things are good. Andrew Mayne: But end of the day, Google is an ad tech company. Andrew Mayne: And that's the thing that always kind of, you know, I, you know, using their API and stuff is not as too concerned, although, but I do think about like, man, if I'm using their chatbot, that whatever, that is my thinking is that their where their incentives are aligned. Sam Altman: Google Search was an amazing product for a long time. Sam Altman: it does feel to me like it's degraded. Sam Altman: But, you know, there was like a time where there were lots of ads, but I still thought it was the best thing on the Internet. Sam Altman: I mean, I love Google search. Sam Altman: So I don't like, it's clearly possible to be a good ad driven company, but and I, like, respect a lot of things Google has done, but there are obviously issues too. Andrew Mayne: Yeah. Andrew Mayne: the Apple model, as an Apple user, I liked was I know paying a lot for my phone, but I know they're not trying to cram all these things in it. Andrew Mayne: They did iAds, which was, you know, not terribly effective, which probably showed you their heart was really not in it. Sam Altman: Their heart was really not in it. Andrew Mayne: Yeah. Andrew Mayne: So it's gonna be interesting. Andrew Mayne: I guess we just have to keep watching and seeing this and we start to think, man, know, ChatGPT is really pushing this. Andrew Mayne: I need to start wondering about this. Sam Altman: Anything we do, we obviously need to just be like crazy upfront and clear about. Andrew Mayne: So we had an issue, there was a model update and then the thing that happened was apparently the model was trying to be a little bit too pleasing, was trying to be a little bit too agreeable. Andrew Mayne: And that brings up the human AI interaction as people are using these systems more and developing these relationships with that. Andrew Mayne: Like, do you see the shape of that coming and what's OpenAI's position on personality? Sam Altman: One of the big mistakes of the social media era was the feed algorithms had a bunch of unintended negative consequences on society as a whole and maybe even individual users. Sam Altman: Although they were doing the thing that a user wanted or someone thought that user wanted in the moment, which is get them to, like, keep spending time on the site. Sam Altman: And that was the was the big misalignment of of social media. Sam Altman: And I think there were a lot of other things like, you know, making people upset kind of gets them stuck on more than being like happy and content. Sam Altman: And I always knew that there'd be like new problems in the world with AI. Sam Altman: Where the thing that, you know, there'd be like something that was, like, misaligned in a not obvious way. Sam Altman: But definitely one of the first ones that we experienced was if you ask a user what they for one given response versus and then you try to, like, build a model that is most helpful to the user. Sam Altman: And you show a user, say, two responses, which one's more helpful to you? Sam Altman: On any given thing, you might wanna model to behave one way, but over the course of, you know, all your interaction with an AI, that might not match up. Sam Altman: You know, you can see and we did see these problems where if you pay too much attention to the user signals and a lot of other things that we talked about in our our postmortem, but that I think this is just, an interesting one. Sam Altman: On the short horizon, you kind of don't get the behavior that the user most wants or is most helpful or useful or healthy to a user in the long run. Sam Altman: So, you know, maybe the analogy to filter bubbles is going to be AIs that are, you know, helpful to a user in a short amount horizon, but not over a long horizon. Andrew Mayne: Why, I think a sign of that was DALL-E 3, which I thought technically was a really capable model, but they all kinda started to be one kind of genre of image. Andrew Mayne: And and and all kinda like an HDR sort of style, and was that from doing that sort of comparisons where users said, looking in just these two things in isolation, I prefer this one better? Sam Altman: I don't remember for DALL-E 3, but I would assume so. Andrew Mayne: Yeah. Andrew Mayne: Which I think it's gotten better. Andrew Mayne: New image model is like The new image model is fantastic. Andrew Mayne: Crazy good. Andrew Mayne: Yeah. Andrew Mayne: Yeah. Andrew Mayne: And I can only imagine where that's gonna go from here. Andrew Mayne: So when you're building these things and you're increasing usage, and that's always been sort of a problem, the new image model comes out and you have to restrict usage and you have to have, like you have Sora, which you can only have a certain amount of compute to do that, illustrates the big problem everybody's facing, which is compute. Andrew Mayne: And so to address this, we heard about Project Stargate, which has a very cool name and it involves computers. Andrew Mayne: Other than that, I think a lot of people are going in their price tag, you know, half a trillion dollars. Andrew Mayne: People are going like, wait. Andrew Mayne: What? Andrew Mayne: What what is the simple description I give to my mom about Stargate? Sam Altman: I think it's just it's quite simple. Sam Altman: It's an effort to finance and build an unprecedented amount of compute. Sam Altman: It's totally true that people we don't have enough compute to let people do what they want. Sam Altman: But if people knew what we could do with more compute, they would want way, way more. Sam Altman: So there's this incredibly huge gap between what we could what we can offer the world today and what we could offer the world with 10 times more compute or someday, hopefully, a 100 times more compute. Sam Altman: And a thing that is different about AI than other technologies I've worked on or at least AI at the scale of delivering it usefully to hundreds of millions, billions of people around the world is just how big the infrastructure investment has to be. Sam Altman: And and so Stargate is an effort to pull a lot of capital and technology and operational expertise together to build the infrastructure to go deliver the next generation of services to all the people who want them and make intelligence as abundant and cheap as possible. Andrew Mayne: So it is a massive project, global project. Andrew Mayne: We talked before, one of the partners is The UAE. Andrew Mayne: You're working at that. Andrew Mayne: You're working with other governments around the world on this. Andrew Mayne: One of the considerations is, you know, one, been asked on social media, half a trillion dollars, 500,000,000,000. Andrew Mayne: Do you have the money? Sam Altman: Don't literally have it sitting in the bank account today, Andrew Mayne: but we are Is it in the room right now? Sam Altman: It's not the room today. Sam Altman: But we will deploy it over the next not even that many years Andrew Mayne: Okay. Sam Altman: you know, unless something, like, really goes wrong and it turns out we can't build these computers. Sam Altman: I'm confident that people are are good for it. Sam Altman: I went recently to the first site that we're building out in Abilene. Sam Altman: That'll be about, you know, roughly 10% of all of of all of the initial commitment to Stargate, the the sort of 500,000,000,000. Sam Altman: It's incredible to see. Sam Altman: Yeah. Sam Altman: It is a like, I knew in my head what a order of gigawatt scale site looks like. Sam Altman: But then to go see one being built and the, like, thousands of people running around doing construction and going to, like, you know, stand inside the rooms where the GPUs are getting installed and just, like, look at how complex the whole system is and the speed with which it's going is quite something. Sam Altman: We'll have more to share about the next sites soon, but there's a great quote about a pencil, just like a standard, you know, wood and graphite pencil and how no one person Yeah. Sam Altman: Could build it. Sam Altman: And and it's it's this like magic of capitalism. Andrew Mayne: Mhmm. Sam Altman: It's miracle really that like that the world gets coordinated to do these things. Sam Altman: And and standing inside of the first Stargate site, I was really just thinking about the the global complexity that it took to get these racks of GPUs running. Sam Altman: You know, when you get your phone out and you type something into ChatGPT and you get the answer back, you you probably this point, you probably don't even think that's, like, particularly surprising. Sam Altman: You just expect it to work. Sam Altman: There was a time, maybe the first time you tried it, we're like, that is really amazing. Sam Altman: But the work that happened over the last thousand or at least many hundreds of years of people working incredibly hard to get these hard won scientific insights and then to build the engineering and the companies and the complex supply chains and kind of reconfigure the world that had to happen to get this, like, rack of magic put somewhere. Sam Altman: Think about all the stuff that went into that. Sam Altman: The, you know, that and trace it all the way back to people that were just, like, digging rocks out of the ground and seeing what happened so that you now get to just, you know, type something into ChatGPT and it does something for you. Andrew Mayne: I read a behind the scenes story about the development of Project Stargate and the international partnerships, particularly The UAE, and that Elon Musk had tried to derail that. Andrew Mayne: And what have you seen? Andrew Mayne: What have you heard? Andrew Mayne: What's the take on that? Sam Altman: I had said, I think also externally, but at least internally after the election that I didn't think Elon was going to abuse his power in the government to unfairly compete. Sam Altman: And I regret to say I was wrong about that. Sam Altman: I mean, I don't like being wrong in general, but mostly I just think it's really unfortunate for the country that he would do these things, and I didn't think I genuinely didn't think he was going to. Sam Altman: I'm grateful that the administration has really done the right thing and stuck up to that kind of behavior. Sam Altman: But yeah, it sucks. Andrew Mayne: Well, think the thing that's changed, and I think Greg Brockman had just talked about this, where there was a couple years ago where people thought, like, okay, whoever gets there first is the winner, and that's it, and the game is over. Andrew Mayne: And now we realize there are great AI labs elsewhere. Andrew Mayne: Like Anthropic is building great tools. Andrew Mayne: I think Google's really got its game up. Andrew Mayne: There's good stuff happening everywhere, and it's not gonna be that one person runs away with I agree. Andrew Mayne: And so it seems Sam Altman: I yeah. Sam Altman: The the example that I like the most is the discovery of AI was analogous to this, not perfect, but close, to the discovery of the transistor in many surprising number of ways. Sam Altman: But many companies are gonna build great things on that, and then eventually it's gonna, like, seep into almost all products. Sam Altman: But you won't think about using transistors all the time. Sam Altman: So, yeah, I think a lot of people are gonna build really successful companies built on this incredible scientific discovery. Sam Altman: And I wish Elon would be less zero sum about it. Andrew Mayne: Yeah. Sam Altman: Or negative sum. Andrew Mayne: I think the pie is just gonna get bigger and bigger if we think about that. Andrew Mayne: I was just at an energy conference, and it was interesting talking to the people who were involved in energy production and stuff and hyperscaling, the term they used for this was a topic. Andrew Mayne: And that does bring up, like, the energy requirements. Andrew Mayne: I know that for, Grok 3, apparently, I guess they had to put generators in the parking lot to be able to train that model. Andrew Mayne: And that's the question is, like, how where is the energy gonna come from? Andrew Mayne: Money, I understand. Andrew Mayne: Energy, to think of when we talk about the scale of energy needed. Sam Altman: I think kinda everywhere. Sam Altman: Right. Sam Altman: I think it's a big mix right now. Sam Altman: Eventually, I think a lot of I'm very excited about advanced nuclear, both fission and fusion. Sam Altman: But for now, I think it's it's a whole mix of the entire portfolio. Sam Altman: Right. Sam Altman: Gas, solar, I mean, really nuclear, everything. Andrew Mayne: So all of the above and stuff. Andrew Mayne: Yeah. Andrew Mayne: I was talking to people that were some of them worked in areas like in Alberta where they said we have a lot of access to energy and not as much for it there, etcetera. Andrew Mayne: And now this is just this total picture I hadn't even thought about. Sam Altman: You know, traditionally, it's very hard to move energy around the world. Sam Altman: Most kinds. Sam Altman: But if you exchange energy for intelligence and then move the intelligence around the world, it's much easier. Sam Altman: So you could put the giant training center or even the big inference clusters in a lot of places and then just, like, ship the output over the Internet. Andrew Mayne: There was a speaker at at opening. Andrew Mayne: I came to an event, and it was somebody who's working, think it was the James Webb Space Telescope. Andrew Mayne: And he talked about his biggest bottleneck was they're about to get all of this, you know, terabytes of data, but he doesn't have enough scientists to work on it. Andrew Mayne: Doesn't have enough people to go through the data. Andrew Mayne: And here we have these answers about the universe, whatever in front of us, and it's like a big data problem. Sam Altman: Yeah. Sam Altman: I've always joked that one thing we should do when we have enough money, when OpenAI has enough money, is just build a gigantic particle accelerator and solve high energy physics once and for all. Sam Altman: Cause I think that'd be like a triumphant, wonderful thing. Sam Altman: But I wonder what are the odds that a really, really smart AI could look at the data we currently have Andrew Mayne: Mhmm. Sam Altman: With no more data, no bigger particle accelerator and just figure it out. Sam Altman: It's not impossible. Sam Altman: Yeah. Sam Altman: And and, yeah, so there's this question of like, okay, there's already a lot of data out there. Sam Altman: There's a lot of smart people in the world, but we don't know how far intelligence can go. Sam Altman: With no more experiments, how much more could we figure out? Andrew Mayne: I remember reading some of talk about how in early nineteen nineties somebody had found like a form of Ozempic, alright, and presented it to like a drug company to this and they said, nah, we're gonna pass on that. Andrew Mayne: And that's been a life changing drug for people, like for people who've just basically done chronic obesity, whatever, it's gonna improve the quality of life, you think, this was sitting there for twenty five years. Sam Altman: I suspect there's a lot of other examples that we'll find where maybe we already have existing drugs that we know do something good, but they're reusable in some other big way or with a couple of small modifications, we are very close to something great. Sam Altman: And it's been very heartening to hear from scientists using the even the current generation models for this kind of work. Andrew Mayne: So it sounds like one of the things we're gonna need though for next generation models is models that understand physics and chemistry and stuff. Andrew Mayne: Is Sora sort of a stab at that? Sam Altman: I mean, it'll understand like Newtonian physics. Sam Altman: I don't know if it'll help us with discovering new chemistry and sort of like new, like novel physics or novel theoretical physics or whatever you'd like. Sam Altman: But I think I'm optimistic that the techniques we use for the reasoning models will help us with those things a lot. Andrew Mayne: Okay. Andrew Mayne: And what is the short definition of how a reasoning model works versus just me asking GPT 4.1 something? Sam Altman: So the GPT models can reason a little bit. Sam Altman: And in fact, one of the one of the things that got people really excited in the early days of the GPT models was you could get better performance by telling the model, let's think step by step. Sam Altman: And it would then just output text that was thinking step by step and get a better answer, which was sort of amazing that that worked at all. Sam Altman: The reasoning models are just pushing that much further. Andrew Mayne: So it's the idea of like, when it's able to break the question down, it can send more time on each step. Sam Altman: When you ask me something, a question, I, if it's a really easy question, I might just fire back like almost on reflex with the answer. Sam Altman: But if it's a harder question, I might think in my head and have like my internal monologue go and say, well, I could do this or that or maybe maybe, you know, this will be clearer. Sam Altman: I'm not sure about that. Sam Altman: And I could like backtrack and retrace my steps. Sam Altman: And then when I finished thinking and I've, you know, been thinking in English, I can then, you know, make some bullet points and then kind of like, I'll put an answer to you in English. Andrew Mayne: One of the interesting things I've observed now when I use the app, if I ask a Deep Research question or something and I go away on my lock screen, I get the it's still processing and thinking about it. Andrew Mayne: And I heard somebody, another company, forget who was, was using a metric of how long something spent. Andrew Mayne: I think it was Anthropic. Andrew Mayne: Like I said, hey, this model actually spent like fifteen minutes or thirty minutes or whatever length of time to think about a thing, which is a good metric, but it needs to actually give you the right answer. Andrew Mayne: And I thought that was sort of just interesting paradigm of Sam Altman: One thing I have been surprised by is people are surprisingly willing to wait for a great answer. Sam Altman: Yeah. Sam Altman: Even if models are think for a while. Sam Altman: All of my instincts have been, you know, the instant response is the thing that matters and users hate to wait. Sam Altman: And for a lot of stuff, that's true. Sam Altman: But for hard problems with a really good answer, people are quite willing to wait. Andrew Mayne: Yeah. Andrew Mayne: So we have all these tools, all these things. Andrew Mayne: So far, I'm using my phone. Andrew Mayne: And now OpenAI just announced that you guys are building hardware. Andrew Mayne: You had the video with you and Jony Ive talking about you guys been talking about and collaborating for a couple years. Andrew Mayne: Obviously, you can't I mean, well, I could ask you this. Andrew Mayne: Is is it on you right now? Sam Altman: No. Sam Altman: It is not. Andrew Mayne: Alright. Sam Altman: It's gonna be a while. Andrew Mayne: Okay. Sam Altman: We're gonna try to do something at like a crazy high level of quality and that that does not come fast. Sam Altman: But computers, software and hardware, just the way we think of current computers, were designed for a world without AI. Sam Altman: And now we're in, like, a very different world, and what you want out of hardware and software is changing quite rapidly. Sam Altman: You might want something that is way more aware of its environment, that has way more context in your life. Sam Altman: You might wanna interact with it in a different way than, like, typing and looking at a screen. Sam Altman: And we've been exploring that for a while, and we've got a couple of ideas we're really quite excited about. Sam Altman: I think it will take time for people to get used to what it means to use a computer in this kind of a world because it is so different now. Sam Altman: But if you, like, really trusted an AI to understand all the context of your life and your question and make good judgments on your behalf where you could, like, have it sit in a meeting, listen to the whole meeting, know what it was, like, allowed to share with who and what it shouldn't share with anyone and, you know, kind of what your preferences would be. Sam Altman: And then you ask it one question, you trust that it's gonna go do the right follow ups with the right people and do like you can then imagine a totally different kind of how you use a computer to get done what you want. Andrew Mayne: So kind of the way we interact with ChatGPT is kind of kind of inform the device. Sam Altman: I mean, could also say that the way we interact with ChatGPT was informed by the previous generation of devices. Sam Altman: So I think it is this sort of like co evolving thing, but yeah, I hope so. Andrew Mayne: One of the things that made the phone so ubiquitous was the fact that I can be in public and look at the screen. Andrew Mayne: I can be in private and have a phone call and talk to it. Andrew Mayne: And I think that's one of the challenges for new devices is that trying to bridge that gap between what we use in public and private. Sam Altman: Phones are unbelievable things. Sam Altman: I mean, they are really fantastic for a lot of reasons. Sam Altman: And you can imagine one new device that you could use everywhere, but also like there's some things that I do do differently publicly and probably like at home, I've got great stereo system built in the music. Sam Altman: When I'm walking the world, I use AirPods and that doesn't bother me. Sam Altman: Yeah. Sam Altman: So I think there are things that are different in the public and private use case, but the general purposeness, I agree is important. Andrew Mayne: Yeah. Andrew Mayne: It follows you with it. Andrew Mayne: So nothing yet until maybe next year. Sam Altman: It's gonna be a while. Andrew Mayne: Alright. Sam Altman: It will be worth the wait, hope, but it's gonna be a while. Andrew Mayne: Okay. Andrew Mayne: I'm I'm excited and curious. Andrew Mayne: I have thoughts. Andrew Mayne: So if you're giving advice to a 25 old right now, what do tell them? Sam Altman: I mean, the obvious tactical stuff is probably what you'd expect me to say, like learn how to use AI tools. Sam Altman: It's funny how quickly the world went from telling, you know, the average 20 year old, 25 year old learn to program Mhmm. Sam Altman: To program it doesn't matter. Sam Altman: Learn to use AI tools. Sam Altman: I wonder what will be next, but of course, there will be something next. Sam Altman: But that's that's very good tactical advice. Sam Altman: And then on the sort of like broader front, I believe that skills like resilience, adaptability, creativity, figuring out what other people want. Sam Altman: I think these are all surprisingly learnable. Sam Altman: And it's not as easy as say, like, go practice using ChatGPT, but it is doable. Sam Altman: And those are the kind of skills that I think will pay off a lot in the next, you know, couple of decades. Andrew Mayne: And would you say same thing for 45 year olds is just learn how to use it in your role now? Andrew Mayne: Yeah. Andrew Mayne: Probably. Andrew Mayne: Whenever we have whatever your personal definition of AGI, will more people be working for OpenAI after then or before? Andrew Mayne: More. Andrew Mayne: More. Andrew Mayne: So, yeah. Andrew Mayne: I see a lot of online people like, oh, they're they're so good. Andrew Mayne: Why are they hiring people? Andrew Mayne: I'm like, because computers can't do everything. Andrew Mayne: They're not gonna do everything. Sam Altman: The slightly longer answer with more than one word is that there will be more people, but each of them will do vastly more than what one person did, you know, in the pre AGI times. Andrew Mayne: Right. Andrew Mayne: Which is the goal of technology. Andrew Mayne: Yeah.
```

### üìã Action Plan

Based on this video transcript between Andrew Mayne and Sam Altman, I'll create an action plan focused on effectively incorporating AI (specifically ChatGPT) into daily life and parenting, as these were key themes discussed.

SUMMARY:
The video discusses the current and future state of AI, particularly focusing on ChatGPT's role in parenting, education, and society. Sam Altman shares insights about how AI will integrate into future generations' lives and its evolving capabilities.

PREREQUISITES:
- Access to ChatGPT or similar AI tools
- Basic understanding of AI capabilities
- Open mindset to incorporating AI tools responsibly

ACTION PLAN:

1. Initial AI Integration (1-2 weeks)
   - Set up ChatGPT account
   - Familiarize yourself with basic functions
   - Start with simple queries and interactions
   Time: 2-3 hours initial setup

2. Parenting Support Implementation (Ongoing)
   - Create a list of common parenting questions
   - Use ChatGPT for developmental stage information
   - Document helpful responses for future reference
   Time: 30 minutes daily

3. Educational Enhancement (1 month to establish)
   - Develop structured learning objectives
   - Create AI-assisted lesson plans
   - Combine traditional teaching methods with AI support
   Time: 2-3 hours weekly setup

4. Building Healthy AI Boundaries (2 weeks)
   - Establish usage guidelines for family
   - Set specific times for AI interaction
   - Create balance between AI and human interaction
   Time: 1-2 hours planning

5. Regular Assessment and Adjustment (Monthly)
   - Review effectiveness of AI usage
   - Adjust guidelines as needed
   - Stay updated on new AI capabilities
   Time: 1 hour monthly

EXPECTED OUTCOMES:
- More informed parenting decisions
- Enhanced learning experiences
- Efficient problem-solving capabilities
- Balanced integration of AI in daily life

PITFALLS TO AVOID:
1. Over-reliance on AI for decision-making
2. Neglecting human interaction and relationships
3. Using AI as a replacement for professional advice
4. Failing to verify information from multiple sources
5. Not setting appropriate boundaries for AI usage

REQUIRED TOOLS:
- ChatGPT subscription (or similar AI tool)
- Device with internet access
- Note-taking system
- Time management tools
- Regular access to educational resources

This plan should be adjusted based on individual needs and circumstances, with a focus on responsible and beneficial AI integration.

---

## Analysis Details

**Tools Used:** youtube_transcript
